spark.password.factory = com.huawei.spark.client.ClientDecode
spark.rdd.compress = false
spark.yarn.report.interval = 1000
spark.streaming.receiver.writeAheadLog.maxFailures = 3
spark.shuffle.io.sendBuffer = -1
spark.sql.ui.retainedExecutions = 1000
spark.shuffle.compress = true
spark.dynamicAllocation.minExecutors = 0
spark.task.maxFailures = 4
spark.locality.wait.rack = 3000
spark.shuffle.io.maxRetries = 12
spark.kryoserializer.buffer.max = 64MB
spark.streaming.kafka.maxRetries = 1
spark.yarn.am.extraJavaOptions = -Dlog4j.configuration=./log4j-executor.properties -Djava.security.auth.login.config=./jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./kdc.conf -Djdk.tls.ephemeralDHKeySize=2048
spark.yarn.am.waitTime = 100s
spark.streaming.receiverRestartDelay = 2000
spark.shuffle.memoryFraction = 0.2
spark.speculation.multiplier = 1.5
spark.ui.retainedJobs = 1000
spark.sql.bigdata.register.dialect = org.apache.spark.sql.CarbonInternalSqlParser
spark.rpc.io.threads = 0
spark.shuffle.manager = SORT
spark.streaming.kafka.direct.lifo = false
spark.reducer.maxSizeInFlight = 48MB
spark.streaming.ui.retainedBatches = 1000
spark.dynamicAllocation.initialExecutors = 0
spark.akka.timeout = 100s
spark.executor.memory = 1G
spark.shuffle.sort.bypassMergeThreshold = 200
spark.scheduler.minRegisteredResourcesRatio = 0.8
spark.rpc.numRetries = 3
spark.sql.bigdata.register.preExecutionRule = org.apache.spark.sql.execution.EnsureRowFormats$,org.apache.spark.sql.hive.CarbonPrivCheck
spark.eventLog.compress = false
spark.sql.autoBroadcastJoinThreshold = 10485760
spark.port.maxRetries = 16
spark.akka.threads = 4
spark.serializer.objectStreamReset = 100
spark.yarn.am.memory = 512M
spark.executor.extraClassPath = 
spark.python.worker.reuse = true
spark.sql.broadcastTimeout = 300
spark.admin.acls = admin
spark.sql.parquet.filterPushdown = true
spark.yarn.containerLauncherMaxThreads = 25
spark.yarn.queue = default
spark.io.compression.snappy.blockSize = 32KB
spark.sql.dialect = org.apache.spark.sql.hive.huawei.BigSQLDialect
spark.driver.cores = 1
spark.network.timeoutInterval = 60
spark.security.hideInfo.enabled = true
spark.driver.extraLibraryPath = /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/lib/native
spark.sql.inMemoryColumnarStorage.partitionPruning = true
spark.files.fetchTimeout = 60
spark.ui.view.acls = 
spark.sql.bigdata.register.extendedResolutionRule = org.apache.spark.sql.hive.CarbonPreInsertionCasts
spark.storage.cachedPeersTtl = 60000
spark.buffer.size = 65536
spark.eventQueue.size = 1000000
spark.shuffle.io.retryWait = 5s
spark.streaming.blockInterval = 200ms
spark.ui.retainedStages = 1000
spark.driver.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=./log4j-executor.properties -Djava.security.auth.login.config=./jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./kdc.conf -Djetty.version=x.y.z -Dorg.xerial.snappy.tempdir=/opt/huawei/Bigdata/tmp -Djava.io.tmpdir=/opt/huawei/Bigdata/tmp -Dcarbon.properties.filepath=./carbon.properties -Djdk.tls.ephemeralDHKeySize=2048
spark.eventLog.overwrite = true
spark.dynamicAllocation.cachedExecutorIdleTimeout = 120
spark.shuffle.spill.compress = true
spark.monitor.jmx.serverClass = com.huawei.spark.monitor.JMXServer
spark.random.port.min = 23000
spark.speculation.quantile = 0.75
spark.executor.cores = 1
spark.dynamicAllocation.maxExecutors = 2048
spark.shuffle.sort.kvChunkSize = 4194304
spark.rpc.io.numConnectionsPerPeer = 1
spark.thriftserver.zookeeper.namespace = /sparkthriftserver
spark.storage.maxReplicationFailures = 1
spark.sql.infer.filters = true
spark.task.cpus = 1
spark.speculation.interval = 100
spark.sql.bigdata.show.query = true
spark.executor.userClassPathFirst = false
spark.shuffle.file.buffer = 32KB
spark.shuffle.service.enabled = false
spark.sql.planner.skewJoin = false
spark.sql.sources.maxConcurrentWrites = 5
spark.sql.authorization.enabled = true
spark.scheduler.revive.interval = 1000
spark.buffer.pageSize = 16m
spark.sql.inMemoryColumnarStorage.batchSize = 10000
spark.network.timeout = 240s
spark.sql.bigdata.register.optimizeRuleTail = org.apache.spark.sql.optimizer.ResolveCarbonFunctions
spark.sql.shuffle.partitions = 200
spark.hbase.obtainToken.enabled = false
spark.storage.memoryMapThreshold = 2m
spark.shuffle.io.numConnectionsPerPeer = 1
spark.kryo.registrationRequired = false
spark.streaming.driver.writeAheadLog.rollingIntervalSecs = 60
spark.ui.retainedDeadExecutors = 100
spark.logConf = false
spark.authenticate.enableSaslEncryption = true
spark.akka.frameSize = 128
spark.yarn.archive = hdfs://hacluster/user/spark/lib/V100R002C80SPC200/spark-assembly-1.5.1-hadoop2.7.2.zip
spark.executor.heartbeatInterval = 10000
spark.yarn.am.cores = 1
spark.broadcast.blockSize = 4096
spark.yarn.scheduler.heartbeat.interval-ms = 3000
spark.sql.tungsten.enabled = true
spark.streaming.driver.writeAheadLog.maxFailures = 3
spark.rpc.lookupTimeout = 120s
spark.sql.saveBroadcastTables.enabled = true
spark.sql.parquet.compression.codec = snappy
spark.storage.memoryFraction = 0.6
spark.mapOutputStatus.maxShuffleMapStatusSize = 256M
spark.rpc.retry.wait = 3s
spark.dynamicAllocation.enabled = false
spark.files.overwrite = false
spark.locality.wait.node = 3000
spark.driver.extraClassPath = 
spark.hadoop.validateOutputSpecs = true
spark.streaming.receiver.blockStoreTimeout = 30
spark.driver.userClassPathFirst = false
spark.kryo.referenceTracking = true
spark.deploy.zookeeper.url = 189.211.68.235:24002,189.211.68.223:24002,189.211.69.32:24002
spark.akka.heartbeat.pauses = 6000s
spark.scheduler.executorTaskBlacklistTime = 60000
spark.locality.wait = 3000
spark.streaming.concurrentJobs = 1
spark.dynamicAllocation.executorIdleTimeout = 60s
spark.broadcast.factory = org.apache.spark.broadcast.TorrentBroadcastFactory
spark.storage.unrollFraction = 0.2
spark.rpc.connect.threads = 64
spark.sql.parquet.binaryAsString = false
spark.dynamicAllocation.schedulerBacklogTimeout = 1s
spark.solr.obtainToken.enabled = false
spark.sql.caseSensitive = false
spark.io.compression.lz4.blockSize = 32KB
spark.sql.inMemoryColumnarStorage.compressed = true
spark.scheduler.mode = FIFO
spark.ui.https.enabledAlgorithms = TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_DHE_DSS_WITH_AES_256_GCM_SHA384,TLS_DHE_RSA_WITH_AES_256_CBC_SHA256,TLS_DHE_DSS_WITH_AES_256_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_DSS_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_DSS_WITH_AES_128_GCM_SHA256
spark.yarn.dist.innerarchives = hdfs://hacluster/user/spark/lib/V100R002C80SPC200/kettle.zip#kettle
spark.shuffle.consolidateFiles = false
spark.ui.https.protocol = TLSv1.1,TLSv1.2
spark.shuffle.spill.batchSize = 10000
spark.local.dir = #{tmp_dir}/spark/localDir
spark.sql.sources.parallelPartitionDiscovery.threshold = 32
spark.rpc.askTimeout = 120s
spark.executor.extraLibraryPath = /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/lib/native
spark.shuffle.spill = true
spark.ui.customErrorPage = true
spark.sql.jobname.length = 50
spark.ui.memoryFraction = 0.1
spark.beeline.principal = spark/hadoop.hadoop.com@HADOOP.COM
spark.locality.wait.process = 3000
spark.sql.parquet.cacheMetadata = true
spark.sql.broadcastTables.sizeInBytes = 104857600
spark.authenticate.secret = 
spark.sql.bigdata.register.strategyRule = org.apache.spark.sql.hive.CarbonStrategy,org.apache.spark.sql.hive.CarbonDDLStrategy
spark.scheduler.maxRegisteredResourcesWaitingTime = 30000
spark.driver.memory = 512M
spark.logout.enabled = true
spark.shuffle.sasl.timeout = 120s
spark.ui.port = 0
spark.ui.killEnabled = true
spark.io.compression.codec = lz4
spark.speculation = false
spark.yarn.access.namenodes = hdfs://hacluster,hdfs://hacluster
spark.sql.bigdata.initFunction = org.apache.spark.sql.CarbonEnv
spark.modify.acls = 
spark.sql.planner.skewJoin.threshold = 100000
spark.shuffle.io.receiveBuffer = -1
spark.sql.unifySearch.query.threshold = 10000
spark.eventLog.dir = hdfs://hacluster/sparkJobHistory
spark.serializer = org.apache.spark.serializer.JavaSerializer
spark.kryoserializer.buffer = 64KB
spark.localExecution.enabled = false
spark.sql.bigdata.register.analyseRule = org.apache.spark.sql.hive.acl.CarbonAccessControlRules
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s
spark.shuffle.io.mode = NIO
spark.shuffle.io.preferDirectBufs = true
spark.streaming.receiver.writeAheadLog.enable = false
spark.executor.periodicGC.interval = 30min
spark.acls.enable = true
spark.yarn.submit.waitAppCompletion = true
spark.eventLog.group.size = 30
spark.cleaner.periodicGC.interval = 30min
spark.authenticate = true
spark.ui.threadDumpsEnabled = true
spark.streaming.blockQueueSize = 10
spark.random.port.max = 23999
spark.shuffle.service.port = 27337
spark.sql.parquet.int96AsTimestamp = true
spark.inputFormat.cache.enabled = true
spark.eventLog.enabled = true
spark.streaming.unpersist = true
spark.executor.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=./log4j-executor.properties -Djava.security.auth.login.config=./jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./kdc.conf -Dcarbon.properties.filepath=./carbon.properties -Djdk.tls.ephemeralDHKeySize=2048
